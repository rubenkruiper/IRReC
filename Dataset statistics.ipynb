{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d9d22a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, glob\n",
    "from textblob import TextBlob\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40eccb11",
   "metadata": {},
   "source": [
    "Document dict contatins a list of dicts:\n",
    "* content, retrieved_by, id\n",
    "* meta: {text, doc_title, split_size, split_id, SPaR_labels, filtered_SPaR_labels, cluster_filtered, cluster_neighbours}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367acba4",
   "metadata": {},
   "source": [
    "* Do I want to count tokens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba2ee0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_labels(counter, len_counter, labels):\n",
    "    for label in labels:\n",
    "        counter[label] += 1\n",
    "        len_counter[len(TextBlob(label).words)] += 1\n",
    "    return counter, len_counter\n",
    "\n",
    "def process_doc(doc_path, doc_titles_seen, doc_count, doc_pages_for_title, doc_title_count, passage_cnt, u_passage_cnt, passage_len_cnt,\\\n",
    "            sent_cnt, sent_len_cnt, word_cnt, word_len_cnt, spar_cnt, spar_word_len_cnt, f_spar_cnt, \\\n",
    "            f_spar_word_len_cnt, nn_cnt, nn_word_len_cnt, f_nn_cnt, f_nn_word_len_cnt):\n",
    "    with open(doc_path, 'r') as f:\n",
    "        passages = [json.loads(l) for l in f.readlines()]\n",
    "        page_nrs = []\n",
    "        for passage in passages:\n",
    "            doc_title = passage['meta']['doc_title']\n",
    "            if doc_title in doc_titles_seen:\n",
    "                raise ValueError(f\"Document title already seen: {doc_title}\\n Document: {doc_path}\\n\") \n",
    "                \n",
    "            if \"ocab\" in doc_title:\n",
    "                raise ValueError(f\"Document is a vocabulary: {doc_title}\\n Document: {doc_path}\\n\") \n",
    "\n",
    "            page_nr = int(passage['id'].rsplit('##', 2)[1])\n",
    "            doc_title = passage['meta']['doc_title']\n",
    "            text = passage['content']\n",
    "            spar = [l for l in passage['meta']['SPaR_labels'][1:-1].split(\", \") if l]\n",
    "            f_spar = [l for l in passage['meta']['filtered_SPaR_labels'][1:-1].split(\", \") if l]\n",
    "            cluster = [l for l in passage['meta']['cluster_neighbours'][1:-1].split(\", \") if l]\n",
    "            f_cluster = [l for l in passage['meta']['cluster_filtered'][1:-1].split(\", \") if l]\n",
    "\n",
    "            blob = TextBlob(text)\n",
    "            sents = blob.sentences\n",
    "            # during indexing passages are cut off to be 100 words max\n",
    "            sent_word_lens = []\n",
    "            for sent in sents:\n",
    "                sent_word_lens.append(len(sent.words)) if len(sent.words)  < 100 else sent_word_lens.append(100)\n",
    "            words = [str(w) for w in blob.words]\n",
    "\n",
    "            # counting\n",
    "            doc_title_count[doc_title] += 1\n",
    "            passage_cnt += 1\n",
    "            u_passage_cnt[text] += 1\n",
    "            passage_len_cnt[len(text)] += 1\n",
    "\n",
    "\n",
    "            for sent in sents:\n",
    "                sent_cnt[sent] += 1\n",
    "\n",
    "            for num_words in sent_word_lens:\n",
    "                sent_len_cnt[num_words] += 1\n",
    "\n",
    "            for w in words:\n",
    "                word_cnt[w] += 1\n",
    "                word_len_cnt[len(w)] += 1\n",
    "\n",
    "            spar_cnt, spar_word_len_cnt = count_labels(spar_cnt, spar_word_len_cnt, spar)\n",
    "            f_spar_cnt, f_spar_word_len_cnt = count_labels(f_spar_cnt, f_spar_word_len_cnt, f_spar)\n",
    "            nn_cnt, nn_word_len_cnt = count_labels(nn_cnt, nn_word_len_cnt, cluster)\n",
    "            f_nn_cnt, f_nn_word_len_cnt = count_labels(f_nn_cnt, f_nn_word_len_cnt, f_cluster)\n",
    "                \n",
    "        page_nrs.append(page_nr)\n",
    "        doc_pages_for_title[max(page_nrs)] += 1\n",
    "        doc_titles_seen.append(doc_title)\n",
    "\n",
    "    return doc_count, doc_pages_for_title, doc_title_count, passage_cnt, u_passage_cnt, passage_len_cnt,\\\n",
    "            sent_cnt, sent_len_cnt, word_cnt, word_len_cnt, spar_cnt, spar_word_len_cnt, f_spar_cnt, \\\n",
    "            f_spar_word_len_cnt, nn_cnt, nn_word_len_cnt, f_nn_cnt, f_nn_word_len_cnt, doc_titles_seen\n",
    "\n",
    "\n",
    "def statistics_per_split(converted_files_directory):\n",
    "    # grab processed .json files\n",
    "    if not converted_files_directory.endswith(\"/\"):\n",
    "        converted_files_directory = converted_files_directory + \"/\"\n",
    "    documents = glob.glob(converted_files_directory + \"**/*.json\", recursive=True)\n",
    "\n",
    "    # init counters\n",
    "    doc_count = 0\n",
    "    duplicates = []\n",
    "    doc_title_count = Counter()\n",
    "    doc_pages_for_title = Counter()\n",
    "    \n",
    "    passage_cnt = 0 \n",
    "    u_passage_cnt = Counter() \n",
    "    passage_len_cnt = Counter()\n",
    "    sent_cnt = Counter()\n",
    "    sent_len_cnt = Counter()\n",
    "    word_cnt = Counter()\n",
    "    word_len_cnt = Counter()\n",
    "    \n",
    "    spar_cnt = Counter()\n",
    "    spar_word_len_cnt = Counter()\n",
    "    f_spar_cnt = Counter()\n",
    "    f_spar_word_len_cnt = Counter()\n",
    "    nn_cnt = Counter()\n",
    "    nn_word_len_cnt = Counter()\n",
    "    f_nn_cnt = Counter()\n",
    "    f_nn_word_len_cnt = Counter()\n",
    "    \n",
    "\n",
    "    \n",
    "    doc_titles_seen = []\n",
    "    for doc_path in tqdm(documents):\n",
    "        doc_count += 1\n",
    "        try:\n",
    "            doc_count, doc_pages_for_title, doc_title_count, passage_cnt, u_passage_cnt, passage_len_cnt,\\\n",
    "            sent_cnt, sent_len_cnt, word_cnt, word_len_cnt, spar_cnt, spar_word_len_cnt, f_spar_cnt, \\\n",
    "            f_spar_word_len_cnt, nn_cnt, nn_word_len_cnt, f_nn_cnt, f_nn_word_len_cnt, doc_titles_seen = process_doc(doc_path, doc_titles_seen, doc_count, doc_pages_for_title, doc_title_count, passage_cnt, u_passage_cnt, passage_len_cnt,\n",
    "                                        sent_cnt, sent_len_cnt, word_cnt, word_len_cnt, spar_cnt, spar_word_len_cnt, f_spar_cnt, \n",
    "                                        f_spar_word_len_cnt, nn_cnt, nn_word_len_cnt, f_nn_cnt, f_nn_word_len_cnt)\n",
    "        except ValueError:\n",
    "            # doc_title already seen\n",
    "            duplicates.append(doc_path)\n",
    "            continue\n",
    "    \n",
    "    print(f\"Skipped {len(duplicates)} duplicates\")\n",
    "    return doc_count, doc_pages_for_title, doc_title_count, passage_cnt, u_passage_cnt, passage_len_cnt,\\\n",
    "            sent_cnt, sent_len_cnt, word_cnt, word_len_cnt, spar_cnt, spar_word_len_cnt, f_spar_cnt, \\\n",
    "            f_spar_word_len_cnt, nn_cnt, nn_word_len_cnt, f_nn_cnt, f_nn_word_len_cnt, duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2dadf65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 471/471 [17:29<00:00,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 70 duplicates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "directory = \"datavolume/ir_data/pdf_converted\"\n",
    "\n",
    "doc_count, doc_pages_for_title, doc_title_count, passage_cnt, u_passage_cnt, passage_len_cnt, \\\n",
    "            sent_cnt, sent_len_cnt, word_cnt, word_len_cnt, spar_cnt, spar_word_len_cnt, f_spar_cnt, \\\n",
    "            f_spar_word_len_cnt, nn_cnt, nn_word_len_cnt, f_nn_cnt, f_nn_word_len_cnt, duplicates = statistics_per_split(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "988bc89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nr of documents: 407\n",
      "% of documents ≥ 100 pages: 13.27%\n",
      "Mean doc length (pages): 62.85\n",
      "Standard deviation length (pages): 62.07\n",
      "Shortest doc (pages): 3\n",
      "Longest doc (pages): 549\n"
     ]
    }
   ],
   "source": [
    "page_lengths = [v for page_lens, c in doc_pages_for_title.items() for v in [page_lens]*c]\n",
    "print(\"nr of documents: {}\".format(sum(doc_pages_for_title.values())))\n",
    "print(\"% of documents ≥ 100 pages: {:.2f}%\".format(\n",
    "    (len([v for v in page_lengths if v > 100]) / len(page_lengths))*100))\n",
    "print(\"Mean doc length (pages): {:.2f}\".format(np.mean(page_lengths)))\n",
    "print(\"Standard deviation length (pages): {:.2f}\".format(np.std(page_lengths)))\n",
    "print(\"Shortest doc (pages): {}\".format(np.min(page_lengths)))\n",
    "print(\"Longest doc (pages): {}\".format(np.max(page_lengths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cb7b0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passages: 87998 (87998 unique) passages found in 471 documents\n",
      "407 unique doc titles and total:  87998\n",
      "Expected nr of words (100 * passages): 8799800\n"
     ]
    }
   ],
   "source": [
    "print(f\"Passages: {passage_cnt} ({sum(u_passage_cnt.values())} unique) passages found in {doc_count} documents\")\n",
    "print(len(doc_title_count.keys()), \"unique doc titles and total: \", sum(doc_title_count.values()))\n",
    "\n",
    "print(f\"Expected nr of words (100 * passages): {passage_cnt * 100}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab98408c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Sentences: 296701\n",
      "Unique sentences: 247766\n",
      "% of sents ≥ 80 words: 3.81%\n",
      "Mean sentence length (words): 23.93\n",
      "Standard deviation length (words): 20.10\n",
      "Shortest sentence: 1\n",
      "Longest sentence: 100\n"
     ]
    }
   ],
   "source": [
    "# sentences\n",
    "sent_lenghts = [v for sent_lens, c in sent_len_cnt.items() for v in [sent_lens]*c if v > 0]\n",
    "print(\"# Sentences: {}\".format(len(sent_lenghts)))\n",
    "print(\"Unique sentences: {}\".format(len(sent_cnt.keys())))\n",
    "cut_off_len = 80\n",
    "print(\"% of sents ≥ {} words: {:.2f}%\".format(cut_off_len,\n",
    "    (len([v for v in sent_lenghts if v > cut_off_len]) / len(sent_lenghts)) *100))\n",
    "\n",
    "print(\"Mean sentence length (words): {:.2f}\".format(np.mean(sent_lenghts)))\n",
    "print(\"Standard deviation length (words): {:.2f}\".format(np.std(sent_lenghts)))\n",
    "print(\"Shortest sentence: {}\".format(np.min(sent_lenghts)))\n",
    "print(\"Longest sentence: {}\".format(np.max(sent_lenghts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c61653c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 126359, total nr of `words`: 7105215\n",
      "% of words ≥ 10 characters: 15.46%\n",
      "Mean word length (chars): 5.11\n",
      "Standard deviation length (chars): 3.40\n",
      "Shortest word (chars): 1\n",
      "Longest word (chars): 387\n"
     ]
    }
   ],
   "source": [
    "# words\n",
    "print(f\"Vocabulary size: {len(word_cnt.keys())}, total nr of `words`: {sum(word_cnt.values())}\")\n",
    "word_lenghts = [v for word_lens, c in word_len_cnt.items() for v in [word_lens]*c]\n",
    "cut_off_len = 10\n",
    "print(\"% of words ≥ {} characters: {:.2f}%\".format(cut_off_len,\n",
    "    (sum([v for v in word_lenghts if v > cut_off_len]) / sum(word_lenghts))* 100))\n",
    "print(\"Mean word length (chars): {:.2f}\".format(np.mean(word_lenghts)))\n",
    "print(\"Standard deviation length (chars): {:.2f}\".format(np.std(word_lenghts)))\n",
    "print(\"Shortest word (chars): {}\".format(np.min(word_lenghts)))\n",
    "print(\"Longest word (chars): {}\".format(np.max(word_lenghts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a72932b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce53a52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPaR label vocab size: 561405, total nr of labels: 2221966\n",
      "Mean length (words): 2.21\n",
      "Standard deviation length (words): 1.73\n",
      "Shortest label (words): 1\n",
      "Longest label (words: 68\n",
      "\n",
      "Filtered SPaR label vocab size: 440430, total nr of labels: 1684454\n",
      "Mean length (words): 2.14\n",
      "Standard deviation length (words): 1.51\n",
      "Shortest label (words): 1\n",
      "Longest label (words): 68\n",
      "\n",
      "Cluster label vocab size: 342265, total nr of labels: 5017893\n",
      "Mean length (words): 2.64\n",
      "Standard deviation length (words): 1.49\n",
      "Shortest label (words): 1\n",
      "Longest label (words): 65\n",
      "\n",
      "Filtered cluster label vocab size: 278262, total nr of labels: 2793678\n",
      "Mean length (words): 2.94\n",
      "Standard deviation length (words): 1.81\n",
      "Shortest label (words): 1\n",
      "Longest label (words): 65\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# label lengths\n",
    "spar = [v for label_len, c in spar_word_len_cnt.items() for v in [label_len]*c if v > 0]\n",
    "print(f\"SPaR label vocab size: {len(spar_cnt.keys())}, total nr of labels: {sum(spar_cnt.values())}\")\n",
    "print(\"Mean length (words): {:.2f}\".format(np.mean(spar)))\n",
    "print(\"Standard deviation length (words): {:.2f}\".format(np.std(spar)))\n",
    "print(\"Shortest label (words): {}\".format(np.min(spar)))\n",
    "print(\"Longest label (words: {}\\n\".format(np.max(spar)))\n",
    "\n",
    "f_spar = [v for label_len, c in f_spar_word_len_cnt.items() for v in [label_len]*c if v > 0]\n",
    "print(f\"Filtered SPaR label vocab size: {len(f_spar_cnt.keys())}, total nr of labels: {sum(f_spar_cnt.values())}\")\n",
    "print(\"Mean length (words): {:.2f}\".format(np.mean(f_spar)))\n",
    "print(\"Standard deviation length (words): {:.2f}\".format(np.std(f_spar)))\n",
    "print(\"Shortest label (words): {}\".format(np.min(f_spar)))\n",
    "print(\"Longest label (words): {}\\n\".format(np.max(f_spar)))\n",
    "\n",
    "nn = [v for label_len, c in nn_word_len_cnt.items() for v in [label_len]*c if v > 0]\n",
    "print(f\"Cluster label vocab size: {len(nn_cnt.keys())}, total nr of labels: {sum(nn_cnt.values())}\")\n",
    "print(\"Mean length (words): {:.2f}\".format(np.mean(nn)))\n",
    "print(\"Standard deviation length (words): {:.2f}\".format(np.std(nn)))\n",
    "print(\"Shortest label (words): {}\".format(np.min(nn)))\n",
    "print(\"Longest label (words): {}\\n\".format(np.max(nn)))\n",
    "\n",
    "f_nn = [v for label_len, c in f_nn_word_len_cnt.items() for v in [label_len]*c if v > 0]\n",
    "print(f\"Filtered cluster label vocab size: {len(f_nn_cnt.keys())}, total nr of labels: {sum(f_nn_cnt.values())}\")\n",
    "print(\"Mean length (words): {:.2f}\".format(np.mean(f_nn)))\n",
    "print(\"Standard deviation length (words): {:.2f}\".format(np.std(f_nn)))\n",
    "print(\"Shortest label (words): {}\".format(np.min(f_nn)))\n",
    "print(\"Longest label (words): {}\\n\".format(np.max(f_nn)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223de2b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c749d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae3a0a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e750db8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "131e39c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BS 4422 (2005).json\n",
      "BS EN ISO 1182-2010--[Reaction to fire tests for products - no Combustibility].json\n",
      "BS 6100-6-2008.json\n",
      "BS EN 81-58 (2018).json\n",
      "BS 476-7-1997--[2019-08-27--02-01-39 PM].json\n",
      "BS 7974 (2019).json\n",
      "BS EN 13501-5-2016.json\n",
      "BS EN 594-2011.json\n",
      "BS EN 1993-1-2 (2005).json\n",
      "BS 8313 (1997).json\n",
      "BS EN 1365-3-2000--Beams.json\n",
      "Approved Document B - 2010.json\n",
      "BS EN 520-2004+A1-2009--[2019-09-09--05-29-22 PM].json\n",
      "BS EN 1365-5-2004--[2019-07-25--04-50-45 PM].json\n",
      "BS 6100-0-2010.json\n",
      "BS EN 1365-3-2000--[2019-07-25--04-47-42 PM].json\n",
      "BS EN 12114-2000--[Air permeability of builing elements. Laboratory tests].json\n",
      "BS EN ISO 9346-2007--[2019-09-16--10-40-16 AM].json\n",
      "BS EN 1993-1-2 (2005)-UK National Annex to Eurocode 3.json\n",
      "BS EN 1634-3-2004--[2019-09-09--05-23-56 PM].json\n",
      "BS EN 1634-1 (2014) + A1 2018.json\n",
      "BS 8218-1998.json\n",
      "DD ENV 1363-3-2000--Verification of furnace.json\n",
      "BS EN 13501-1-2018.json\n",
      "BS 6100-3-2007.json\n",
      "BS EN 1366-2 (2015).json\n",
      "BS 476-3 (2004).json\n",
      "BS EN 832-2000--[Thermal performance of residential buildings].json\n",
      "BS EN 12101-3 (2015).json\n",
      "BS EN 1366-8 (2004).json\n",
      "BS 476-11 (1982).json\n",
      "BS EN 322-1993--[2020-04-15--09-10-05 PM].json\n",
      "BS ISO 15686-2-2012--[2020-08-07--12-11-15 PM].json\n",
      "BS EN 1365-6-2004--Stairs.json\n",
      "BS EN 1634-2 (2008).json\n",
      "BS EN ISO 9239-1-2010--[2020-03-17--12-52-16 PM].json\n",
      "BS 476-20-1987--[2019-08-27--12-28-14 PM].json\n",
      "BS EN 13238-2010--[2020-02-17--12-11-44 PM].json\n",
      "BS EN ISO 11925-2-2010--[2020-03-17--12-43-36 PM].json\n",
      "BS EN 13823-2010+A1-2014--[2019-07-24--11-16-41 AM].json\n",
      "BS EN 13501-2 (2016).json\n",
      "BS ISO 16000-6-2011--[2019-10-02--03-44-19 PM].json\n",
      "BS EN ISO 13788-2012.json\n",
      "BS 7974-2019--[2020-07-13--02-02-30 PM].json\n",
      "BS EN 13501-4-2016.json\n",
      "BS EN 520 (2004) + A1 2009.json\n",
      "BS EN 1365-4-1999--Columns.json\n",
      "BS EN 310-1993-Wood-based panels. Determination of modulus of elasticity in bending and bending strength.json\n",
      "BS 476-31.1-1983--[2019-09-09--04-41-51 PM].json\n",
      "BS 476-6 (1989) + A1 2009.json\n",
      "BS EN 1195-1998 [Timber structures. Test methods. Performance of structural floor decking].json\n",
      "BS 476-23-1987--Components contribution.json\n",
      "BS EN 1366-8-2004--Smoke Extraction Ducts.json\n",
      "BS EN 1090-2-2018--[Execution of steel structures and aluminium structures. Technical requirements for steel structures].json\n",
      "BS 4422-2005.json\n",
      "BS 5268-6.1-1996-.json\n",
      "DD CEN-TS 1187-2012--[2020-02-17--11-50-47 AM].json\n",
      "BS EN 14509-2013-Self-supporting double skin metal faced insulating panels. Factory made products. Specifications.json\n",
      "BS 476-22 (1987).json\n",
      "BS EN 1365-2-2014--[2019-07-25--04-47-10 PM].json\n",
      "BS EN 1365-1-2012--General requirements.json\n",
      "BS EN 596-1995--[Timber structures- Impact] (1).json\n",
      "BS EN 13501-3 (2005) + A1 2009.json\n",
      "BS 8212-1995 Dry lining and partitioning using gypsum plasterboard.json\n",
      "BS EN 13829-2001--[Determination of air permeability of buildings. Fan method].json\n",
      "BS 476-24-1987, ISO 6944-1985--[2019-08-27--12-34-14 PM].json\n",
      "BS EN 12524-2000--[Building materials and properties - Hygrothermal].json\n",
      "BS EN ISO 12572-2016--Hygrothermal performance.json\n",
      "BS 476-21 (1987).json\n",
      "BS EN 15725 (2010).json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(d.rsplit(\"/\",1)[1]) for d in duplicates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8693c5a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
