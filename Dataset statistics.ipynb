{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d9d22a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, glob\n",
    "from textblob import TextBlob\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40eccb11",
   "metadata": {},
   "source": [
    "Document dict contatins a list of dicts:\n",
    "* content, retrieved_by, id\n",
    "* meta: {text, doc_title, split_size, split_id, SPaR_labels, filtered_SPaR_labels, cluster_filtered, cluster_neighbours}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367acba4",
   "metadata": {},
   "source": [
    "* Do I want to count tokens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba2ee0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_labels(counter, len_counter, labels):\n",
    "    for label in labels:\n",
    "        counter[label] += 1\n",
    "        len_counter[len(TextBlob(label).words)] += 1\n",
    "    return counter, len_counter\n",
    "\n",
    "def process_doc(doc_path, doc_titles_seen, doc_count, doc_pages_for_title, doc_title_count, passage_cnt, u_passage_cnt, passage_len_cnt,\\\n",
    "            sent_cnt, sent_len_cnt, word_cnt, word_len_cnt, spar_cnt, spar_word_len_cnt, f_spar_cnt, \\\n",
    "            f_spar_word_len_cnt, nn_cnt, nn_word_len_cnt, f_nn_cnt, f_nn_word_len_cnt):\n",
    "    with open(doc_path, 'r') as f:\n",
    "        passages = [json.loads(l) for l in f.readlines()]\n",
    "        page_nrs = []\n",
    "        for passage in passages:\n",
    "            doc_title = passage['meta']['doc_title']\n",
    "            if doc_title in doc_titles_seen:\n",
    "                raise ValueError(f\"Document title already seen: {doc_title}\\n Document: {doc_path}\\n\") \n",
    "                \n",
    "            page_nr = int(passage['id'].rsplit('##', 2)[1])\n",
    "            doc_title = passage['meta']['doc_title']\n",
    "            text = passage['content']\n",
    "            spar = [l for l in passage['meta']['NER_labels'][1:-1].split(\", \") if l]\n",
    "            f_spar = [l for l in passage['meta']['filtered_NER_labels'][1:-1].split(\", \") if l]\n",
    "            cluster = [l for l in passage['meta']['filtered_NER_labels_domains'][1:-1].split(\", \") if l]\n",
    "            f_cluster = [l for l in passage['meta']['neighbours'][1:-1].split(\", \") if l]\n",
    "\n",
    "            blob = TextBlob(text)\n",
    "            sents = blob.sentences\n",
    "            # during indexing passages are cut off to be 100 words max\n",
    "            sent_word_lens = []\n",
    "            for sent in sents:\n",
    "                sent_word_lens.append(len(sent.words)) if len(sent.words)  < 100 else sent_word_lens.append(100)\n",
    "            words = [str(w) for w in blob.words]\n",
    "\n",
    "            # counting\n",
    "            doc_title_count[doc_title] += 1\n",
    "            passage_cnt += 1\n",
    "            u_passage_cnt[text] += 1\n",
    "            passage_len_cnt[len(text)] += 1\n",
    "\n",
    "\n",
    "            for sent in sents:\n",
    "                sent_cnt[sent] += 1\n",
    "\n",
    "            for num_words in sent_word_lens:\n",
    "                sent_len_cnt[num_words] += 1\n",
    "\n",
    "            for w in words:\n",
    "                word_cnt[w] += 1\n",
    "                word_len_cnt[len(w)] += 1\n",
    "\n",
    "            spar_cnt, spar_word_len_cnt = count_labels(spar_cnt, spar_word_len_cnt, spar)\n",
    "            f_spar_cnt, f_spar_word_len_cnt = count_labels(f_spar_cnt, f_spar_word_len_cnt, f_spar)\n",
    "            nn_cnt, nn_word_len_cnt = count_labels(nn_cnt, nn_word_len_cnt, cluster)\n",
    "            f_nn_cnt, f_nn_word_len_cnt = count_labels(f_nn_cnt, f_nn_word_len_cnt, f_cluster)\n",
    "                \n",
    "        page_nrs.append(page_nr)\n",
    "        doc_pages_for_title[max(page_nrs)] += 1\n",
    "        doc_titles_seen.append(doc_title)\n",
    "\n",
    "    return doc_count, doc_pages_for_title, doc_title_count, passage_cnt, u_passage_cnt, passage_len_cnt,\\\n",
    "            sent_cnt, sent_len_cnt, word_cnt, word_len_cnt, spar_cnt, spar_word_len_cnt, f_spar_cnt, \\\n",
    "            f_spar_word_len_cnt, nn_cnt, nn_word_len_cnt, f_nn_cnt, f_nn_word_len_cnt, doc_titles_seen\n",
    "\n",
    "\n",
    "def statistics_per_split(converted_files_directory):\n",
    "    # grab processed .json files\n",
    "\n",
    "    documents = [x for x in converted_files_directory.glob(\"*.json\")]\n",
    "\n",
    "    # init counters\n",
    "    doc_count = 0\n",
    "    duplicates = []\n",
    "    doc_title_count = Counter()\n",
    "    doc_pages_for_title = Counter()\n",
    "    \n",
    "    passage_cnt = 0 \n",
    "    u_passage_cnt = Counter() \n",
    "    passage_len_cnt = Counter()\n",
    "    sent_cnt = Counter()\n",
    "    sent_len_cnt = Counter()\n",
    "    word_cnt = Counter()\n",
    "    word_len_cnt = Counter()\n",
    "    \n",
    "    spar_cnt = Counter()\n",
    "    spar_word_len_cnt = Counter()\n",
    "    f_spar_cnt = Counter()\n",
    "    f_spar_word_len_cnt = Counter()\n",
    "    nn_cnt = Counter()\n",
    "    nn_word_len_cnt = Counter()\n",
    "    f_nn_cnt = Counter()\n",
    "    f_nn_word_len_cnt = Counter()\n",
    "    \n",
    "\n",
    "    \n",
    "    doc_titles_seen = []\n",
    "    for doc_path in tqdm(documents):\n",
    "        doc_count += 1\n",
    "        try:\n",
    "            doc_count, doc_pages_for_title, doc_title_count, passage_cnt, u_passage_cnt, passage_len_cnt,\\\n",
    "            sent_cnt, sent_len_cnt, word_cnt, word_len_cnt, spar_cnt, spar_word_len_cnt, f_spar_cnt, \\\n",
    "            f_spar_word_len_cnt, nn_cnt, nn_word_len_cnt, f_nn_cnt, f_nn_word_len_cnt, doc_titles_seen = process_doc(doc_path, doc_titles_seen, doc_count, doc_pages_for_title, doc_title_count, passage_cnt, u_passage_cnt, passage_len_cnt,\n",
    "                                        sent_cnt, sent_len_cnt, word_cnt, word_len_cnt, spar_cnt, spar_word_len_cnt, f_spar_cnt, \n",
    "                                        f_spar_word_len_cnt, nn_cnt, nn_word_len_cnt, f_nn_cnt, f_nn_word_len_cnt)\n",
    "        except ValueError:\n",
    "            # doc_title already seen\n",
    "            duplicates.append(doc_path)\n",
    "            continue\n",
    "    \n",
    "    print(f\"Skipped {len(duplicates)} duplicates\")\n",
    "    return doc_count, doc_pages_for_title, doc_title_count, passage_cnt, u_passage_cnt, passage_len_cnt,\\\n",
    "            sent_cnt, sent_len_cnt, word_cnt, word_len_cnt, spar_cnt, spar_word_len_cnt, f_spar_cnt, \\\n",
    "            f_spar_word_len_cnt, nn_cnt, nn_word_len_cnt, f_nn_cnt, f_nn_word_len_cnt, duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2dadf65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████| 420/420 [44:14<00:00,  6.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 0 duplicates\n"
     ]
    }
   ],
   "source": [
    "directory = Path(\"datavolume/ir_data/foreground_pdf_converted/\")\n",
    "\n",
    "doc_count, doc_pages_for_title, doc_title_count, passage_cnt, u_passage_cnt, passage_len_cnt, \\\n",
    "            sent_cnt, sent_len_cnt, word_cnt, word_len_cnt, spar_cnt, spar_word_len_cnt, f_spar_cnt, \\\n",
    "            f_spar_word_len_cnt, nn_cnt, nn_word_len_cnt, f_nn_cnt, f_nn_word_len_cnt, duplicates = statistics_per_split(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "01fdc03a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "420"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "988bc89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nr of documents: 420\n",
      "% of documents ≥ 100 pages: 14.29%\n",
      "Mean doc length (pages): 63.71\n",
      "Standard deviation length (pages): 64.57\n",
      "Shortest doc (pages): 3\n",
      "Longest doc (pages): 548\n"
     ]
    }
   ],
   "source": [
    "page_lengths = [v for page_lens, c in doc_pages_for_title.items() for v in [page_lens]*c]\n",
    "print(\"nr of documents: {}\".format(sum(doc_pages_for_title.values())))\n",
    "print(\"% of documents ≥ 100 pages: {:.2f}%\".format(\n",
    "    (len([v for v in page_lengths if v > 100]) / len(page_lengths))*100))\n",
    "print(\"Mean doc length (pages): {:.2f}\".format(np.mean(page_lengths)))\n",
    "print(\"Standard deviation length (pages): {:.2f}\".format(np.std(page_lengths)))\n",
    "print(\"Shortest doc (pages): {}\".format(np.min(page_lengths)))\n",
    "print(\"Longest doc (pages): {}\".format(np.max(page_lengths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5cb7b0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passages: 287876 (287876 unique) passages found in 420 documents\n",
      "420 unique doc titles and total:  287876\n",
      "Expected nr of words (100 * passages): 28787600\n"
     ]
    }
   ],
   "source": [
    "print(f\"Passages: {passage_cnt} ({sum(u_passage_cnt.values())} unique) passages found in {doc_count} documents\")\n",
    "print(len(doc_title_count.keys()), \"unique doc titles and total: \", sum(doc_title_count.values()))\n",
    "\n",
    "print(f\"Expected nr of words (100 * passages): {passage_cnt * 100}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ab98408c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Sentences: 741180\n",
      "Unique sentences: 213703\n",
      "% of sents ≥ 80 words: 1.44%\n",
      "Mean sentence length (words): 25.24\n",
      "Standard deviation length (words): 17.52\n",
      "Shortest sentence: 1\n",
      "Longest sentence: 100\n"
     ]
    }
   ],
   "source": [
    "# sentences\n",
    "sent_lenghts = [v for sent_lens, c in sent_len_cnt.items() for v in [sent_lens]*c if v > 0]\n",
    "print(\"# Sentences: {}\".format(len(sent_lenghts)))\n",
    "print(\"Unique sentences: {}\".format(len(sent_cnt.keys())))\n",
    "cut_off_len = 80\n",
    "print(\"% of sents ≥ {} words: {:.2f}%\".format(cut_off_len,\n",
    "    (len([v for v in sent_lenghts if v > cut_off_len]) / len(sent_lenghts)) *100))\n",
    "\n",
    "print(\"Mean sentence length (words): {:.2f}\".format(np.mean(sent_lenghts)))\n",
    "print(\"Standard deviation length (words): {:.2f}\".format(np.std(sent_lenghts)))\n",
    "print(\"Shortest sentence: {}\".format(np.min(sent_lenghts)))\n",
    "print(\"Longest sentence: {}\".format(np.max(sent_lenghts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c61653c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 121161, total nr of `words`: 18712290\n",
      "% of words ≥ 10 characters: 15.68%\n",
      "Mean word length (chars): 5.14\n",
      "Standard deviation length (chars): 3.38\n",
      "Shortest word (chars): 1\n",
      "Longest word (chars): 293\n"
     ]
    }
   ],
   "source": [
    "# words\n",
    "print(f\"Vocabulary size: {len(word_cnt.keys())}, total nr of `words`: {sum(word_cnt.values())}\")\n",
    "word_lenghts = [v for word_lens, c in word_len_cnt.items() for v in [word_lens]*c]\n",
    "cut_off_len = 10\n",
    "print(\"% of words ≥ {} characters: {:.2f}%\".format(cut_off_len,\n",
    "    (sum([v for v in word_lenghts if v > cut_off_len]) / sum(word_lenghts))* 100))\n",
    "print(\"Mean word length (chars): {:.2f}\".format(np.mean(word_lenghts)))\n",
    "print(\"Standard deviation length (chars): {:.2f}\".format(np.std(word_lenghts)))\n",
    "print(\"Shortest word (chars): {}\".format(np.min(word_lenghts)))\n",
    "print(\"Longest word (chars): {}\".format(np.max(word_lenghts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1aa17aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_counts(label_name:str, cnt:Counter, word_len_cnt:Counter):\n",
    "    counts = [v for label_len, c in word_len_cnt.items() for v in [label_len]*c if v > 0]\n",
    "    \n",
    "    print(f\"{label_name} vocab size: {len(cnt.keys())}, total nr of labels: {sum(cnt.values())}\")\n",
    "    print(\"Mean length (words): {:.2f}\".format(np.mean(counts)))\n",
    "    print(\"Standard deviation length (words): {:.2f}\".format(np.std(counts)))\n",
    "    print(\"Shortest label (words): {}\".format(np.min(counts)))\n",
    "    print(\"Longest label (words: {}\\n\".format(np.max(counts)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ce53a52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER vocab size: 568926, total nr of labels: 5840598\n",
      "Mean length (words): 2.58\n",
      "Standard deviation length (words): 3.52\n",
      "Shortest label (words): 1\n",
      "Longest label (words: 340\n",
      "\n",
      "filtered NER vocab size: 42258, total nr of labels: 1629792\n",
      "Mean length (words): 1.27\n",
      "Standard deviation length (words): 0.53\n",
      "Shortest label (words): 1\n",
      "Longest label (words: 9\n",
      "\n",
      "domain NER vocab size: 45894, total nr of labels: 1278640\n",
      "Mean length (words): 1.27\n",
      "Standard deviation length (words): 0.53\n",
      "Shortest label (words): 1\n",
      "Longest label (words: 9\n",
      "\n",
      "NNs vocab size: 46287, total nr of labels: 3307190\n",
      "Mean length (words): 1.48\n",
      "Standard deviation length (words): 0.62\n",
      "Shortest label (words): 1\n",
      "Longest label (words: 9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# label lengths\n",
    "print_counts(\"NER\", spar_cnt, spar_word_len_cnt)\n",
    "print_counts(\"filtered NER\", f_spar_cnt, f_spar_word_len_cnt)\n",
    "print_counts(\"domain NER\", nn_cnt, nn_word_len_cnt)\n",
    "print_counts(\"NNs\", f_nn_cnt, f_nn_word_len_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "223de2b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45893"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([k for k, v in nn_cnt.items() if k.strip()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "99c749d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42257"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([k for k, v in f_spar_cnt.items() if k.strip()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae3a0a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e750db8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "131e39c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len([d for d in duplicates]))\n",
    "[print(d.stem) for d in duplicates]    # these are all the vocabularies... did I remove those?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8693c5a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
