{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7da117f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, urllib\n",
    "import json, random, os\n",
    "\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from difflib import SequenceMatcher\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a06a035",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [q.strip() for q in open(Path(\"data/queries.txt\")).readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db95f51d",
   "metadata": {},
   "source": [
    "In this notebook we run our queries through the IR system for a variety of settings, with the aim to compare annotated results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6380d1",
   "metadata": {},
   "source": [
    "### methods for calling IR system functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f23e999",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_queries(results_as_json, existing_csvs, prediction_type=\"combined_prediction\", top_k=10):\n",
    "    \"\"\"\n",
    "    Method to call the IR system's API (port 8000) with/without QE. Stores the results as a .json file, \n",
    "    then untangles the predictions that were grouped by document to grab the top_k results. These are\n",
    "    the top retrieved passages for the query, which are then stored in a .csv file. \n",
    "    \n",
    "    :return results_as_csv: Path to the specific csv file with the top_k results.\n",
    "    \"\"\"\n",
    "    \n",
    "    results_as_csv = results_as_json.parent.joinpath( results_as_json.stem + \".csv\")\n",
    "    \n",
    "    if os.path.exists(results_as_json):\n",
    "        with open(results_as_json, 'r') as f:\n",
    "            all_query_results = json.load(f)\n",
    "        QUERIES_READY = True\n",
    "    else:\n",
    "        QUERIES_READY = False\n",
    "    \n",
    "    # Actually run the queries\n",
    "    if not QUERIES_READY: # only save query results if the provided json file doesn't exist\n",
    "        all_query_results = {}\n",
    "        for query in queries:\n",
    "            query = query.encode('ascii', errors='ignore').decode()\n",
    "            if query not in all_query_results.keys():\n",
    "                try:\n",
    "                    print(f\"Query: {query}\")\n",
    "                    encoded_q = urllib.parse.quote(query)\n",
    "\n",
    "                    if prediction_type == \"expanded_prediction\":\n",
    "                        dict_returned_by_api = requests.post(f\"http://localhost:8000/qe/{encoded_q}\").json()\n",
    "                    else:\n",
    "                        dict_returned_by_api = requests.post(f\"http://localhost:8000/q/{encoded_q}\").json()\n",
    "                        \n",
    "                    all_query_results[query] = dict_returned_by_api\n",
    "                except:\n",
    "                    print(\"Somehow issue with query: \", query)\n",
    "                    \n",
    "                    \n",
    "        with open(results_as_json, 'w') as f:\n",
    "            json.dump(all_query_results, f)\n",
    "            \n",
    "    # Reformat query output (not grouped by document, but simply ranked list of passages)\n",
    "    untangled, expanded_queries = untangle_combined_predictions(all_query_results, prediction_type)\n",
    "    \n",
    "    # Convert to .csv\n",
    "    # top_k is the number of passsages I want to annotate\n",
    "    column_indices, rows = grab_information_for_manual_judging(untangled, expanded_queries, top_k=top_k)\n",
    "    df = pd.DataFrame(rows, columns=column_indices)\n",
    "    \n",
    "    # grab existing annotations\n",
    "    print(\"Grabbing existing annotations\")\n",
    "#     df = find_existing_annotations(df, existing_csvs)\n",
    "    df.to_csv(results_as_csv, index=False) \n",
    "    \n",
    "    # quickly check how many rows pre-annotated now\n",
    "    print(f\"Nr. annotations found: {len([x[1]['relevance'] for x in df.iterrows() if x[1]['relevance']])}\")\n",
    "    return results_as_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "208ebeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_settings():\n",
    "    \"\"\"\n",
    "    Function to reset the settings of the IR system to whatever default you'd like it to be (change here).\n",
    "    \"\"\"\n",
    "    base_settings = {\n",
    "            'retrieval': {\n",
    "                'ner_url': \"http://spar:8501/\",\n",
    "                'classifier_url': \"http://classifier:8502/\",\n",
    "                \"top_k\": 15\n",
    "            },\n",
    "            'indexing': {\n",
    "                'indexing_type':  'sparse',\n",
    "                'index_name':  'with_DE',\n",
    "                'sparse_settings': {\n",
    "                    'type': 'bm25'\n",
    "                },\n",
    "                'fields_and_weights': {\n",
    "                    'content': 2,\n",
    "                    'doc_title': 1,\n",
    "                    'NER_labels': 0,\n",
    "                    'filtered_NER_labels': 0,\n",
    "                    'filtered_NER_labels_domains': 0,\n",
    "                    'neighbours': 0,\n",
    "                    'bm25_weight': 0\n",
    "                }\n",
    "            },\n",
    "            'query_expansion': {\n",
    "                'haystack_url': \"http://haystack:8500/\",\n",
    "                'prf_weight': 0,\n",
    "                'kg_weight': 0,\n",
    "                'nn_weight': 0\n",
    "            },\n",
    "          \"recreate_sparse_index\": False,\n",
    "          \"recreate_dense_index\": False\n",
    "        }\n",
    "    return base_settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9192da7",
   "metadata": {},
   "source": [
    "### Methods for processing result and reusing annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e25b263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def untangle_combined_predictions(data, pred_type=\"combined_prediction\"):\n",
    "    \"\"\" \n",
    "    We combine the predictions of the IR system per document, which is useful for presenting results in a UI. But\n",
    "    for annotation this combination obscures which results are ranked high; thus we untangle them for annotations.\n",
    "    \"\"\"\n",
    "    untangled_data = {}\n",
    "    expanded_queries = []\n",
    "    for q in data:\n",
    "        untangled_data[q] = []\n",
    "        for doc_score, document in data[q][pred_type]:\n",
    "            doc_title_retrieved = 0\n",
    "            for retrieved_passage in document['contents']:\n",
    "                if retrieved_passage['retrieved_by'] == ['doc_title']:\n",
    "                    doc_title_retrieved = retrieved_passage['score']\n",
    "                    continue\n",
    "                retrieved_passage['score'] += doc_title_retrieved\n",
    "                untangled_data[q].append(retrieved_passage)\n",
    "        \n",
    "        untangled_data[q] = sorted(untangled_data[q], key=lambda x: x['score'], reverse=True)\n",
    "        if pred_type == \"expanded_prediction\":\n",
    "            expanded_queries.append(data[q][\"expanded_query\"])\n",
    "    return untangled_data, expanded_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88091e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_document_identifier(title):\n",
    "    \"\"\" Grab the document identifier if we can \"\"\"\n",
    "    if not title:\n",
    "        return \"\"\n",
    "    match = None\n",
    "    potential_id = \"\"\n",
    "    if title.startswith(\"BS\") or title.startswith(\"NA\") or title.startswith(\"+\") or \\\n",
    "        title.startswith(\"P\") or title.startswith(\"D\"):\n",
    "        match = re.match(r\"[A-Z +]+(to)?[A-Z ]+([\\d\\-+:]+)([A-Z ]+[\\d\\-+:]+)?\", title)\n",
    "    elif title[0].isdigit():\n",
    "        match = re.match(r'[0-9: +]([\\dA-Z\\-+:]+)([A-Z ]+[\\d\\-+:]+)?')\n",
    "    elif title.startswith(\"Eurocode\"):\n",
    "        match = re.match(r\"Euro([\\w :])+([\\d\\-+:]+)([A-Z ]+[\\d\\-+:]+)?\", title)\n",
    "        \n",
    "\n",
    "    if match:\n",
    "        end_idx = match.end()\n",
    "        potential_id = title[:end_idx]\n",
    "\n",
    "    return potential_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72fe5b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_information_for_manual_judging(untangled_dict, expanded_queries, top_k=9):\n",
    "    \"\"\"\n",
    "    Function to grab the information we'd like to check/judge manually (columns in a CSV).\n",
    "    \"\"\"\n",
    "    column_indices = [\"relevance\", 'query', 'rank', 'document_id', 'document_title', 'page_nr', 'text']\n",
    "    rows = []\n",
    "    \n",
    "    if expanded_queries:\n",
    "        column_indices.append(\"expanded query\")\n",
    "    \n",
    "    for idx, query in enumerate(untangled_dict.keys()):     \n",
    "        if expanded_queries:\n",
    "            expanded_query = expanded_queries[idx]\n",
    "        \n",
    "        for rank, passage_dict in enumerate(untangled_dict[query][:top_k]):\n",
    "            page_nr = passage_dict['id'].rsplit('##')[1]\n",
    "            text = passage_dict['text']\n",
    "            doc_title = passage_dict['doc_title']\n",
    "            doc_id = find_document_identifier(doc_title)\n",
    "            new_row = [\"\", query, rank+1, doc_id, doc_title, page_nr, text]\n",
    "            \n",
    "            if expanded_queries:\n",
    "                new_row.append(expanded_query)\n",
    "\n",
    "            try:\n",
    "                rows.append(new_row)\n",
    "            except IndexError:\n",
    "                pass\n",
    "                \n",
    "    return column_indices, rows\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1baa695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_existing_annotations(to_be_annotated_df, existing_csv):\n",
    "    \"\"\"\n",
    "    Helper function to reuse either a single CSV annotation file, or a list of them.\n",
    "    \"\"\"\n",
    "    # annotated\n",
    "    if type(existing_csv) == str:\n",
    "        print(f\"=== Grab annotations from {existing_csv} \")\n",
    "        # update from annotations csv    \n",
    "        to_be_annotated_df = reuse_existing_csv_annotations(to_be_annotated_df, existing_csv)\n",
    "    elif type(existing_csv) == list:\n",
    "        for old_csv in existing_csv:\n",
    "            print(f\"=== Grab annotations from {old_csv} \")\n",
    "            # update from annotations csv    \n",
    "            to_be_annotated_df = reuse_existing_csv_annotations(to_be_annotated_df, old_csv)\n",
    "    return to_be_annotated_df\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42999bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reuse_existing_csv_annotations(to_be_annotated_df, existing_csv):\n",
    "    \"\"\"\n",
    "    Re-use annotations found in previously annotated CSV files, so that the pandas DF to be annotated already is \n",
    "    pre-populated with those annotations.\n",
    "    \"\"\"\n",
    "    # load csv to pandas DF\n",
    "    annotations_df = pd.read_csv(existing_csv)\n",
    "    # Add the relevance to specific queries and first 3 retrieved documents\n",
    "    query = \"\"\n",
    "    passage = \"\"\n",
    "    doc_rank = 0\n",
    "    new_rows = []\n",
    "    \n",
    "    for tba_row in to_be_annotated_df.iterrows():\n",
    "        if not 'query' in tba_row[1]:\n",
    "            continue\n",
    "        \n",
    "        query = tba_row[1]['query'].lower()\n",
    "        context = tba_row[1]['text']\n",
    "        if tba_row[1]['relevance'] in ['y', 'n']:\n",
    "            new_rows.append(tba_row[1])\n",
    "            continue\n",
    "            \n",
    "        for row in annotations_df.iterrows():\n",
    "            existing_query = row[1]['query'].lower()\n",
    "            existing_context = row[1]['text']\n",
    "            query_match = sum([m.size for m in SequenceMatcher(None, query, existing_query).get_matching_blocks()])\n",
    "            if query_match > .7 * len(query):\n",
    "                result_match = sum([m.size for m in SequenceMatcher(None, context, existing_context).get_matching_blocks()])\n",
    "                if result_match > .7 * len(context):\n",
    "                    tba_row[1]['relevance'] = row[1]['relevance']\n",
    "        new_rows.append(tba_row[1])\n",
    "    \n",
    "    return pd.DataFrame(new_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d196015",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3063115",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56b1991f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def untangled_metrics(annotated_results, query_type=None):\n",
    "    \"\"\" \n",
    "    Reads a csv file with annotations and returns metrics, also see utils.\n",
    "    :input annotated_results: Filepath (string)\n",
    "    \"\"\"\n",
    "    \n",
    "    annotations = pd.read_csv(annotated_results)\n",
    "    if query_type == None:\n",
    "        query_type= \"expanded_prediction\" if \"QE\" in annotated_results else \"combined_prediction\"\n",
    "    \n",
    "\n",
    "    total_passages = 0\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    total_queries = 0\n",
    "    sum_of_F1s = 0\n",
    "    sum_of_APs = 0\n",
    "    sum_of_MRR = 0\n",
    "\n",
    "    queries = []\n",
    "    for row in annotations.iterrows():\n",
    "        \n",
    "        query = row[1]['query']\n",
    "        if query not in queries:\n",
    "            if queries:\n",
    "                # convert previous bool_list into a score\n",
    "                sum_of_F1s += F1_at_k(query_bool_list, len(query_bool_list))\n",
    "                sum_of_MRR += MRR_query(query_bool_list)\n",
    "                sum_of_APs += AP_query(query_bool_list)\n",
    "            old_len = len(queries)\n",
    "            queries.append(query)\n",
    "            query_bool_list = []\n",
    "\n",
    "        text = row[1]['text']\n",
    "        passage_rank = row[1]['rank']\n",
    "        relevant = True if row[1]['relevance'] in ['Yes', 'yes', 'y', 'Y'] else False\n",
    "        \n",
    "        irrelevant = True if row[1]['relevance'] in ['No', 'no', 'n', 'N'] else False\n",
    "        \n",
    "        if relevant:\n",
    "            total_passages += 1\n",
    "            # True positives\n",
    "            true_positives += 1\n",
    "            query_bool_list.append(True)\n",
    "        elif irrelevant:\n",
    "            total_passages += 1\n",
    "            # False positives\n",
    "            false_positives += 1\n",
    "            query_bool_list.append(False)\n",
    "            \n",
    "    sum_of_F1s += F1_at_k(query_bool_list, len(query_bool_list))\n",
    "    sum_of_MRR += MRR_query(query_bool_list)\n",
    "    sum_of_APs += AP_query(query_bool_list)\n",
    "    \n",
    "    total_queries = len(queries)\n",
    "    avgF1 = sum_of_F1s / total_queries\n",
    "    MAP = sum_of_APs / total_queries\n",
    "    MRR = sum_of_MRR / total_queries\n",
    "\n",
    "    print(f\"Total queries: {total_queries}, Total passages: {total_passages}, Total relevant: {true_positives}\")\n",
    "\n",
    "    print(\"avg F1: {:.2f}\".format(avgF1 * 100))\n",
    "    print(\"MRR: {:.2f} \".format(MRR * 100))\n",
    "    print(\"MAP: {:.2f} \".format(MAP * 100))\n",
    "    print(true_positives)\n",
    "    avgf1_ = \"{:.2f}\".format(avgF1 * 100)\n",
    "    print(avgf1_)\n",
    "    mrr_ = \"{:.2f}\".format(MRR * 100)\n",
    "    print(mrr_)\n",
    "    map_ = \"{:.2f}\".format(MAP * 100)\n",
    "    print(map_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ec659f",
   "metadata": {},
   "source": [
    "### Set experiments and Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5f49a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_csvs = []\n",
    "def reset_existing_csvs():\n",
    "    \"\"\" If you'd want to re-use existing annotations, then provide a list of the .csv files here. \"\"\"\n",
    "    return [] #[fp for fp in Path(\"data/annotated/\").glob(\"R*.csv\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c769facb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up our various IR experiment settings\n",
    "\n",
    "top_results_per_retriever = 10\n",
    "annotation_top_k = 3\n",
    "\n",
    "\n",
    "json_directory = Path(\"data/results/\")\n",
    "spreadsheet_directory = Path(\"/Users/rubenkruiper/Google Drive/[0]_PostDoc/Annotation/\")\n",
    "\n",
    "to_run = [\"sparse.json\",      # regular\n",
    "          \"dense.json\",\n",
    "          \"hybrid.json\",\n",
    "          \"sparse_QE.json\",   # with Query Expansion\n",
    "          \"dense_QE.json\",\n",
    "          \"hybrid_QE.json\",\n",
    "          \"sparse_DE.json\",  # with Document Expansion\n",
    "          \"dense_DE.json\",\n",
    "          \"hybrid_DE.json\",\n",
    "          \"sparse_DE_QE.json\",  # with Query and Document Expansion\n",
    "          \"dense_QE_DE.json\",\n",
    "          \"hybrid_QE_DE.json\"\n",
    "         ]\n",
    "\n",
    "\n",
    "run_settings = []\n",
    "for name in to_run:\n",
    "    base_settings = reset_settings()\n",
    "    base_settings['indexing']['fields_and_weights']['doc_title'] = 1\n",
    "    base_settings['retrieval']['top_k'] = top_results_per_retriever\n",
    "    \n",
    "    if \"sparse\" in name:\n",
    "        base_settings['indexing']['indexing_type'] = 'sparse' \n",
    "        base_settings['indexing']['sparse_settings']['type'] = 'bm25f' if \"DE\" in name else \"bm25\"\n",
    "        base_settings['indexing']['fields_and_weights']['bm25_weight'] = 1\n",
    "        \n",
    "    if \"dense\" in name:\n",
    "        base_settings['indexing']['indexing_type'] = 'dense'\n",
    "        base_settings['indexing']['fields_and_weights']['content'] = 2\n",
    "        \n",
    "    if \"hybrid\" in name:\n",
    "        base_settings['indexing']['indexing_type'] = 'hybrid'\n",
    "        base_settings['indexing']['sparse_settings']['sparse_type'] = 'bm25f' if \"DE\" in name else \"bm25\"\n",
    "        base_settings['indexing']['fields_and_weights']['bm25_weight'] = 6\n",
    "        base_settings['indexing']['fields_and_weights']['content'] = 2\n",
    "        \n",
    "    if \"QE\" in name:\n",
    "        base_settings['query_expansion']['kg_weight'] = 1\n",
    "        base_settings['query_expansion']['nn_weight'] = 2\n",
    "        base_settings['query_expansion']['prf_weight'] = 0\n",
    "        \n",
    "    if \"DE\" in name:\n",
    "        base_settings['indexing']['fields_and_weights']['NER_labels'] = 0\n",
    "        base_settings['indexing']['fields_and_weights']['filtered_NER_labels'] = 1\n",
    "        base_settings['indexing']['fields_and_weights']['filtered_NER_labels_domains'] = 1\n",
    "        base_settings['indexing']['fields_and_weights']['neighbours'] = 1\n",
    "        \n",
    "        base_settings['retrieval']['top_k'] = 10\n",
    "        \n",
    "    run_settings.append(base_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01a7798c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'retrieval': {'ner_url': 'http://spar:8501/',\n",
       "  'classifier_url': 'http://classifier:8502/',\n",
       "  'top_k': 10},\n",
       " 'indexing': {'indexing_type': 'sparse',\n",
       "  'index_name': 'with_DE',\n",
       "  'sparse_settings': {'type': 'bm25'},\n",
       "  'fields_and_weights': {'content': 2,\n",
       "   'doc_title': 1,\n",
       "   'NER_labels': 0,\n",
       "   'filtered_NER_labels': 0,\n",
       "   'filtered_NER_labels_domains': 0,\n",
       "   'neighbours': 0,\n",
       "   'bm25_weight': 1}},\n",
       " 'query_expansion': {'haystack_url': 'http://haystack:8500/',\n",
       "  'prf_weight': 0,\n",
       "  'kg_weight': 0,\n",
       "  'nn_weight': 0},\n",
       " 'recreate_sparse_index': False,\n",
       " 'recreate_dense_index': False}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of a settings dict\n",
    "run_settings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31c931be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'indexing_type': 'sparse',\n",
       " 'sparse_type': 'bm25',\n",
       " 'fields_and_weights': {'content': 2.0,\n",
       "  'doc_title': 1.0,\n",
       "  'NER_labels': 0.0,\n",
       "  'filtered_NER_labels': 0.0,\n",
       "  'filtered_NER_labels_domains': 0.0,\n",
       "  'neighbours': 0.0,\n",
       "  'bm25': 1.0,\n",
       "  'top_k': 10},\n",
       " 'top_k_per_retriever': 10,\n",
       " 'prf_weight': 0.0,\n",
       " 'kg_weight': 0.0,\n",
       " 'nn_weight': 0.0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of updating the system using a specific settings dict\n",
    "requests.post(f\"http://localhost:8000/update_weights/\", json=run_settings[0]).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe510194",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= WORKING ON ===============      data/results/sparse.json\n",
      "prediction_type: combined_prediction\n",
      "annotating top K: 3\n",
      "{'indexing_type': 'sparse', 'sparse_type': 'bm25', 'fields_and_weights': {'content': 2.0, 'doc_title': 1.0, 'NER_labels': 0.0, 'filtered_NER_labels': 0.0, 'filtered_NER_labels_domains': 0.0, 'neighbours': 0.0, 'bm25': 1.0, 'top_k': 10}, 'top_k_per_retriever': 10, 'prf_weight': 0.0, 'kg_weight': 0.0, 'nn_weight': 0.0}\n",
      "Query: Rail-fixed external wall insulation systems\n",
      "Query: Portable waste compactors\n",
      "Query: Wood preservative and fire retardant treatment systems\n",
      "Query: Adjustable struts\n",
      "Grabbing existing annotations\n",
      "Nr. annotations found: 0\n",
      "========= WORKING ON ===============      data/results/dense.json\n",
      "prediction_type: combined_prediction\n",
      "annotating top K: 3\n",
      "{'indexing_type': 'dense', 'sparse_type': 'bm25', 'fields_and_weights': {'content': 2.0, 'doc_title': 1.0, 'NER_labels': 0.0, 'filtered_NER_labels': 0.0, 'filtered_NER_labels_domains': 0.0, 'neighbours': 0.0, 'bm25': 0.0, 'top_k': 10}, 'top_k_per_retriever': 10, 'prf_weight': 0.0, 'kg_weight': 0.0, 'nn_weight': 0.0}\n",
      "Query: Rail-fixed external wall insulation systems\n",
      "Query: Portable waste compactors\n",
      "Query: Wood preservative and fire retardant treatment systems\n",
      "Query: Adjustable struts\n",
      "Grabbing existing annotations\n",
      "Nr. annotations found: 0\n",
      "========= WORKING ON ===============      data/results/hybrid.json\n",
      "prediction_type: combined_prediction\n",
      "annotating top K: 3\n",
      "{'detail': [{'loc': ['body', 'indexing', 'sparse_settings', 'sparse_type'], 'msg': 'extra fields not permitted', 'type': 'value_error.extra'}]}\n",
      "Query: Rail-fixed external wall insulation systems\n",
      "Query: Portable waste compactors\n",
      "Query: Wood preservative and fire retardant treatment systems\n",
      "Query: Adjustable struts\n",
      "Grabbing existing annotations\n",
      "Nr. annotations found: 0\n",
      "========= WORKING ON ===============      data/results/sparse_QE.json\n",
      "prediction_type: expanded_prediction\n",
      "annotating top K: 3\n",
      "{'indexing_type': 'sparse', 'sparse_type': 'bm25', 'fields_and_weights': {'content': 2.0, 'doc_title': 1.0, 'NER_labels': 0.0, 'filtered_NER_labels': 0.0, 'filtered_NER_labels_domains': 0.0, 'neighbours': 0.0, 'bm25': 1.0, 'top_k': 10}, 'top_k_per_retriever': 10, 'prf_weight': 0.0, 'kg_weight': 1.0, 'nn_weight': 2.0}\n",
      "Query: Rail-fixed external wall insulation systems\n",
      "Query: Portable waste compactors\n",
      "Query: Wood preservative and fire retardant treatment systems\n",
      "Query: Adjustable struts\n",
      "Grabbing existing annotations\n",
      "Nr. annotations found: 0\n",
      "========= WORKING ON ===============      data/results/dense_QE.json\n",
      "prediction_type: expanded_prediction\n",
      "annotating top K: 3\n",
      "{'indexing_type': 'dense', 'sparse_type': 'bm25', 'fields_and_weights': {'content': 2.0, 'doc_title': 1.0, 'NER_labels': 0.0, 'filtered_NER_labels': 0.0, 'filtered_NER_labels_domains': 0.0, 'neighbours': 0.0, 'bm25': 0.0, 'top_k': 10}, 'top_k_per_retriever': 10, 'prf_weight': 0.0, 'kg_weight': 1.0, 'nn_weight': 2.0}\n",
      "Query: Rail-fixed external wall insulation systems\n",
      "Query: Portable waste compactors\n",
      "Query: Wood preservative and fire retardant treatment systems\n",
      "Query: Adjustable struts\n",
      "Grabbing existing annotations\n",
      "Nr. annotations found: 0\n",
      "========= WORKING ON ===============      data/results/hybrid_QE.json\n",
      "prediction_type: expanded_prediction\n",
      "annotating top K: 3\n",
      "{'detail': [{'loc': ['body', 'indexing', 'sparse_settings', 'sparse_type'], 'msg': 'extra fields not permitted', 'type': 'value_error.extra'}]}\n",
      "Query: Rail-fixed external wall insulation systems\n",
      "Query: Portable waste compactors\n",
      "Query: Wood preservative and fire retardant treatment systems\n",
      "Query: Adjustable struts\n",
      "Grabbing existing annotations\n",
      "Nr. annotations found: 0\n",
      "========= WORKING ON ===============      data/results/sparse_DE.json\n",
      "prediction_type: combined_prediction\n",
      "annotating top K: 3\n",
      "{'indexing_type': 'sparse', 'sparse_type': 'bm25f', 'fields_and_weights': {'content': 2.0, 'doc_title': 1.0, 'NER_labels': 0.0, 'filtered_NER_labels': 1.0, 'filtered_NER_labels_domains': 1.0, 'neighbours': 1.0, 'bm25': 1.0, 'top_k': 10}, 'top_k_per_retriever': 10, 'prf_weight': 0.0, 'kg_weight': 0.0, 'nn_weight': 0.0}\n",
      "Query: Rail-fixed external wall insulation systems\n",
      "Query: Portable waste compactors\n",
      "Query: Wood preservative and fire retardant treatment systems\n",
      "Query: Adjustable struts\n",
      "Grabbing existing annotations\n",
      "Nr. annotations found: 0\n",
      "========= WORKING ON ===============      data/results/dense_DE.json\n",
      "prediction_type: combined_prediction\n",
      "annotating top K: 3\n",
      "{'indexing_type': 'dense', 'sparse_type': 'bm25f', 'fields_and_weights': {'content': 2.0, 'doc_title': 1.0, 'NER_labels': 0.0, 'filtered_NER_labels': 1.0, 'filtered_NER_labels_domains': 1.0, 'neighbours': 1.0, 'bm25': 0.0, 'top_k': 10}, 'top_k_per_retriever': 10, 'prf_weight': 0.0, 'kg_weight': 0.0, 'nn_weight': 0.0}\n",
      "Query: Rail-fixed external wall insulation systems\n",
      "Query: Portable waste compactors\n",
      "Query: Wood preservative and fire retardant treatment systems\n",
      "Query: Adjustable struts\n",
      "Grabbing existing annotations\n",
      "Nr. annotations found: 0\n",
      "========= WORKING ON ===============      data/results/hybrid_DE.json\n",
      "prediction_type: combined_prediction\n",
      "annotating top K: 3\n",
      "{'detail': [{'loc': ['body', 'indexing', 'sparse_settings', 'sparse_type'], 'msg': 'extra fields not permitted', 'type': 'value_error.extra'}]}\n",
      "Query: Rail-fixed external wall insulation systems\n",
      "Query: Portable waste compactors\n",
      "Query: Wood preservative and fire retardant treatment systems\n",
      "Query: Adjustable struts\n",
      "Grabbing existing annotations\n",
      "Nr. annotations found: 0\n",
      "========= WORKING ON ===============      data/results/sparse_DE_QE.json\n",
      "prediction_type: expanded_prediction\n",
      "annotating top K: 3\n",
      "{'indexing_type': 'sparse', 'sparse_type': 'bm25f', 'fields_and_weights': {'content': 2.0, 'doc_title': 1.0, 'NER_labels': 0.0, 'filtered_NER_labels': 1.0, 'filtered_NER_labels_domains': 1.0, 'neighbours': 1.0, 'bm25': 1.0, 'top_k': 10}, 'top_k_per_retriever': 10, 'prf_weight': 0.0, 'kg_weight': 1.0, 'nn_weight': 2.0}\n",
      "Query: Rail-fixed external wall insulation systems\n",
      "Query: Portable waste compactors\n",
      "Query: Wood preservative and fire retardant treatment systems\n",
      "Query: Adjustable struts\n",
      "Grabbing existing annotations\n",
      "Nr. annotations found: 0\n",
      "========= WORKING ON ===============      data/results/dense_QE_DE.json\n",
      "prediction_type: expanded_prediction\n",
      "annotating top K: 3\n",
      "{'indexing_type': 'dense', 'sparse_type': 'bm25f', 'fields_and_weights': {'content': 2.0, 'doc_title': 1.0, 'NER_labels': 0.0, 'filtered_NER_labels': 1.0, 'filtered_NER_labels_domains': 1.0, 'neighbours': 1.0, 'bm25': 0.0, 'top_k': 10}, 'top_k_per_retriever': 10, 'prf_weight': 0.0, 'kg_weight': 1.0, 'nn_weight': 2.0}\n",
      "Query: Rail-fixed external wall insulation systems\n",
      "Query: Portable waste compactors\n",
      "Query: Wood preservative and fire retardant treatment systems\n",
      "Query: Adjustable struts\n",
      "Grabbing existing annotations\n",
      "Nr. annotations found: 0\n",
      "========= WORKING ON ===============      data/results/hybrid_QE_DE.json\n",
      "prediction_type: expanded_prediction\n",
      "annotating top K: 3\n",
      "{'detail': [{'loc': ['body', 'indexing', 'sparse_settings', 'sparse_type'], 'msg': 'extra fields not permitted', 'type': 'value_error.extra'}]}\n",
      "Query: Rail-fixed external wall insulation systems\n",
      "Query: Portable waste compactors\n",
      "Query: Wood preservative and fire retardant treatment systems\n",
      "Query: Adjustable struts\n",
      "Grabbing existing annotations\n",
      "Nr. annotations found: 0\n"
     ]
    }
   ],
   "source": [
    "# loop through all experiments, updating the settings and running all queries each time\n",
    "for json_file_name, settings in zip(to_run, run_settings):\n",
    "# for json_file_name, settings in zip(to_run[6:], run_settings[6:]):\n",
    "    prediction_type = \"expanded_prediction\" if \"QE\" in json_file_name else \"combined_prediction\" \n",
    "    \n",
    "    json_path = json_directory.joinpath(json_file_name)\n",
    "    print(\"========= WORKING ON ===============     \",  json_path)\n",
    "    print(f\"prediction_type: {prediction_type}\")\n",
    "    print(f\"annotating top K: {annotation_top_k}\")\n",
    "    \n",
    "    # Update the query expansion settings\n",
    "    updated_settings = requests.post(f\"http://localhost:8000/update_weights/\", json=settings).json()\n",
    "    print(updated_settings)\n",
    "\n",
    "    # run the queries and store annotation csv's\n",
    "    existing_csvs = reset_existing_csvs()\n",
    "    csv_file_name = run_queries(json_path, existing_csvs, prediction_type, top_k=annotation_top_k)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6236f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd623b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb0029b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdef1b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
